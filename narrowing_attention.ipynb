{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtekpPODWoqKgHKoaF79Aw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blindTissue/NLP-Project/blob/main/narrowing_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ceL5qMVpgMx"
      },
      "outputs": [],
      "source": [
        "#with no embedding\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config['vocab_size'], config['hidden_size'], padding_idx=config['pad_token_id'])\n",
        "        self.LayerNorm = nn.LayerNorm(config['hidden_size'], eps=config['layer_norm_eps'])\n",
        "        self.dropout = nn.Dropout(config['hidden_dropout_prob'])\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        input_shape = input_ids.size()\n",
        "        seq_length = input_shape[1]\n",
        "        device = input_ids.device\n",
        "        embeddings = self.word_embeddings(input_ids)\n",
        "        embeddings = embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "class BertSelfAttention(nn.Module):\n",
        "      def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        assert config[\"hidden_size\"] % config[\"num_of_attention_heads\"] == 0, \"The hidden size is not a multiple of the number of attention heads\"\n",
        "        self.num_attention_heads = config['num_of_attention_heads']\n",
        "        self.attention_head_size = int(config['hidden_size'] / config['num_of_attention_heads'])\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        self.query = nn.Linear(config['hidden_size'], self.all_head_size)\n",
        "        self.key = nn.Linear(config['hidden_size'], self.all_head_size)\n",
        "        self.value = nn.Linear(config['hidden_size'], self.all_head_size)\n",
        "        self.dense = nn.Linear(config['hidden_size'], config['hidden_size'])\n",
        "\n",
        "      def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "      def forward(self, hidden_states):\n",
        "        mixed_query_layer = self.query(hidden_states)                             \n",
        "        mixed_key_layer = self.key(hidden_states)                                \n",
        "        mixed_value_layer = self.value(hidden_states)                            \n",
        "        \n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)               \n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)                    \n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)                \n",
        "\n",
        "        \n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) \n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        #print(attention_scores.shape) \n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)                    \n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        #build_alibi_tensor(config['num_of_attention_heads'], )                \n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()            \n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) \n",
        "        context_layer = context_layer.view(*new_context_layer_shape)             \n",
        "        \n",
        "        output =  self.dense(context_layer)\n",
        "        \n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config['hidden_size'], config['hidden_size'])\n",
        "        self.LayerNorm = nn.LayerNorm(config['hidden_size'], eps=config['layer_norm_eps'])\n",
        "        self.dropout = nn.Dropout(config['hidden_dropout_prob'])\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "    \n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "        \n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        self_output = self.self(input_tensor)\n",
        "        attention_output = self.output(self_output, input_tensor)\n",
        "        return attention_output\n",
        "\n",
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config['hidden_size'], config['intermediate_size'])\n",
        "        self.intermediate_act_fn = nn.GELU()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "    \n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config['intermediate_size'], config['hidden_size'])\n",
        "        self.LayerNorm = nn.LayerNorm(config['hidden_size'], eps=config['layer_norm_eps'])\n",
        "        self.dropout = nn.Dropout(config['hidden_dropout_prob'])\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "    \n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = BertAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "        \n",
        "    def forward(self, hidden_states):\n",
        "        #print(hidden_states)\n",
        "        attention_output = self.attention(hidden_states)\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config['num_hidden_layers'])])\n",
        "        \n",
        "    def forward(self, hidden_states):\n",
        "        for layer_module in self.layer:\n",
        "            hidden_states = layer_module(hidden_states)\n",
        "        return hidden_states\n",
        "    \n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config['hidden_size'], config['hidden_size'])\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        #print(first_token_tensor.shape)\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "    \n",
        "\n",
        "\n",
        "class BertModel(nn.Module):\n",
        "    def __init__ (self, config):\n",
        "        super().__init__()\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "    def forward(self, input_ids):\n",
        "        embedding_output = self.embeddings(input_ids)\n",
        "        encoded_layers = self.encoder(embedding_output)\n",
        "        sequence_output = encoded_layers\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        return encoded_layers, pooled_output\n",
        "    \n",
        "class BertForBinaryClassification(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config['hidden_dropout_prob'])\n",
        "        self.classifier = nn.Linear(config['hidden_size'], 1)\n",
        "    def forward(self, input_ids):\n",
        "        _, pooled_output = self.bert(input_ids)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "hidden_size = 768\n",
        "max_input_length = 512\n",
        "#final config\n",
        "config = {'hidden_size': hidden_size, 'num_of_attention_heads': 12, 'layer_norm_eps': 1e-12\n",
        "          ,'hidden_dropout_prob': 0.1, 'num_hidden_layers': 12\n",
        "          ,\"intermediate_size\": 3072, \"hidden_act\": \"gelu\", \"hidden_dropout_prob\": 0.1, \"attention_probs_dropout_prob\": 0.1\n",
        "          ,\"max_position_embeddings\": 100, \"type_vocab_size\": 2, \"initializer_range\": 0.02,\n",
        "          \"vocab_size\": 30522, \"pad_token_id\": 0}\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    count = 0\n",
        "    for d in data_loader:\n",
        "        count += 1\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        labels = d[\"labels\"].to(device)\n",
        "        #print(labels)\n",
        "        outputs = torch.sigmoid(model(input_ids=input_ids))\n",
        "        # Calculate the predictions by thresholding at 0.5\n",
        "        preds = (outputs > 0.5).float()\n",
        "\n",
        "        # Use binary cross-entropy loss for binary classification\n",
        "        loss = loss_fn(outputs, labels.unsqueeze(1).float())\n",
        "        correct_predictions += torch.sum(preds == labels.unsqueeze(1))\n",
        "        optimizer.zero_grad()\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    return correct_predictions.cpu().double() / n_examples, np.mean(losses)\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers import get_scheduler\n",
        "\n",
        "\n",
        "def evaluate(model, data_loader, loss_fn, device, n_examples):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            labels = d[\"labels\"].to(device)\n",
        "            outputs = torch.sigmoid(model(input_ids=input_ids))\n",
        "            preds = (outputs > 0.5).float()\n",
        "            loss = loss_fn(outputs, labels.unsqueeze(1).float())\n",
        "            correct_predictions += torch.sum(preds == labels.unsqueeze(1))\n",
        "            losses.append(loss.item())\n",
        "    \n",
        "    return correct_predictions.cpu().double() / n_examples, np.mean(losses)\n",
        "\n",
        "def train(model, train_data_loader, val_data_loader, loss_fn, optimizer, device, scheduler, n_examples_train, n_examples_val, n_epochs):\n",
        "    train_accuracies = []\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
        "        print(\"-\" * 10)\n",
        "\n",
        "        train_accuracy, train_loss = train_epoch(model, train_data_loader, loss_fn, optimizer, device, scheduler, n_examples_train)\n",
        "        val_accuracy, val_loss = evaluate(model, val_data_loader, loss_fn, device, n_examples_val)\n",
        "\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        train_losses.append(train_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Train Accuracy: {train_accuracy:.4f}, Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"Val Accuracy: {val_accuracy:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"train_accuracies\": train_accuracies,\n",
        "        \"train_losses\": train_losses,\n",
        "        \"val_accuracies\": val_accuracies,\n",
        "        \"val_losses\": val_losses\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metrics(history):\n",
        "    train_accuracies = history['train_accuracies']\n",
        "    train_losses = history['train_losses']\n",
        "    val_accuracies = history['val_accuracies']\n",
        "    val_losses = history['val_losses']\n",
        "    epochs = range(1, len(train_accuracies) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot accuracies\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_accuracies, label='Train', marker='o')\n",
        "    plt.plot(epochs, val_accuracies, label='Validation', marker='o')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    # Plot losses\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_losses, label='Train', marker='o')\n",
        "    plt.plot(epochs, val_losses, label='Validation', marker='o')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_metrics(history)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"boolq\")\n",
        "\n",
        "class BoolQADataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for the dataset of BoolQ questions and answers\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, passages, questions, answers, tokenizer, max_len):\n",
        "        self.passages = passages\n",
        "        self.questions = questions\n",
        "        self.answers = answers\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.answers)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        This function is called by the DataLoader to get an instance of the data\n",
        "        :param index:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        passage = str(self.passages[index])\n",
        "        question = self.questions[index]\n",
        "        answer = self.answers[index]\n",
        "\n",
        "        # this is input encoding for your model. Note, question comes first since we are doing question answering\n",
        "        # and we don't wnt it to be truncated if the passage is too long\n",
        "        input_encoding = question + \" [SEP] \" + passage\n",
        "\n",
        "        # encode_plus will encode the input and return a dictionary of tensors\n",
        "        encoded_review = self.tokenizer.encode_plus(\n",
        "            input_encoding,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoded_review['input_ids'][0],  # we only have one example in the batch\n",
        "            'attention_mask': encoded_review['attention_mask'][0],\n",
        "            # attention mask tells the model where tokens are padding\n",
        "            'labels': torch.tensor(answer, dtype=torch.long)  # labels are the answers (yes/no)\n",
        "        }\n",
        "\n",
        "dataset['train'][0]\n",
        "\n",
        "dataset_train_subset = dataset['train'][:8000]\n",
        "dataset_dev_subset = dataset['validation']\n",
        "\n",
        "max_len = 512\n",
        "\n",
        "mytokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "train_dataset = BoolQADataset(\n",
        "        passages=list(dataset_train_subset['passage']),\n",
        "        questions=list(dataset_train_subset['question']),\n",
        "        answers=list(dataset_train_subset['answer']),\n",
        "        tokenizer=mytokenizer,\n",
        "        max_len=max_len\n",
        ")\n",
        "\n",
        "validation_dataset = BoolQADataset(\n",
        "        passages=list(dataset_dev_subset['passage']),\n",
        "        questions=list(dataset_dev_subset['question']),\n",
        "        answers=list(dataset_dev_subset['answer']),\n",
        "        tokenizer=mytokenizer,\n",
        "        max_len=max_len\n",
        "    )\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, 32)\n",
        "validation_dataloader = DataLoader(validation_dataset, 32)\n",
        "\n",
        "print(len(train_dataloader))\n",
        "print(len(validation_dataloader))\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = BertForBinaryClassification(config).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "epochs = 40\n",
        "num_training_steps = epochs * len(train_dataloader)\n",
        "loss_fn = nn.BCELoss().to(device)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "history = train(model, train_dataloader, validation_dataloader, loss_fn, optimizer, device, lr_scheduler, 8000, 103 * 32, epochs)"
      ],
      "metadata": {
        "id": "8a59juWnppQh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}